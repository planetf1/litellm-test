{
    "timestamp": "2025-03-05T08:35:49.454644",
    "model": "amazon.titan-text-express-v1",
    "error": "litellm.UnsupportedParamsError: bedrock does not support parameters: {'tools': [{'type': 'function', 'function': {'name': 'weather_tool', 'description': 'Get the current weather in a location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and country to get weather for'}}, 'required': ['location']}}}], 'tool_choice': 'auto'}, for model=amazon.titan-text-express-v1. To drop these, set `litellm.drop_params=True` or for proxy:\n\n`litellm_settings:\n drop_params: true`\n",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 98, in test_model\n    tool_prompt_response = litellm.completion(\n        model=model_name,\n    ...<2 lines>...\n        tool_choice=\"auto\" # Let the model decide if tool is needed\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1081, in completion\n    optional_params = get_optional_params(\n        functions=functions,\n    ...<31 lines>...\n        **non_default_params,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 2997, in get_optional_params\n    _check_valid_arg(supported_params=supported_params or [])\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 2985, in _check_valid_arg\n    raise UnsupportedParamsError(\n    ...<2 lines>...\n    )\nlitellm.exceptions.UnsupportedParamsError: litellm.UnsupportedParamsError: bedrock does not support parameters: {'tools': [{'type': 'function', 'function': {'name': 'weather_tool', 'description': 'Get the current weather in a location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and country to get weather for'}}, 'required': ['location']}}}], 'tool_choice': 'auto'}, for model=amazon.titan-text-express-v1. To drop these, set `litellm.drop_params=True` or for proxy:\n\n`litellm_settings:\n drop_params: true`\n\n"
}
{
    "timestamp": "2025-03-05T08:35:51.587584",
    "model": "amazon.titan-text-lite-v1",
    "error": "litellm.UnsupportedParamsError: bedrock does not support parameters: {'tools': [{'type': 'function', 'function': {'name': 'weather_tool', 'description': 'Get the current weather in a location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and country to get weather for'}}, 'required': ['location']}}}], 'tool_choice': 'auto'}, for model=amazon.titan-text-lite-v1. To drop these, set `litellm.drop_params=True` or for proxy:\n\n`litellm_settings:\n drop_params: true`\n",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 98, in test_model\n    tool_prompt_response = litellm.completion(\n        model=model_name,\n    ...<2 lines>...\n        tool_choice=\"auto\" # Let the model decide if tool is needed\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 1081, in completion\n    optional_params = get_optional_params(\n        functions=functions,\n    ...<31 lines>...\n        **non_default_params,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 2997, in get_optional_params\n    _check_valid_arg(supported_params=supported_params or [])\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 2985, in _check_valid_arg\n    raise UnsupportedParamsError(\n    ...<2 lines>...\n    )\nlitellm.exceptions.UnsupportedParamsError: litellm.UnsupportedParamsError: bedrock does not support parameters: {'tools': [{'type': 'function', 'function': {'name': 'weather_tool', 'description': 'Get the current weather in a location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and country to get weather for'}}, 'required': ['location']}}}], 'tool_choice': 'auto'}, for model=amazon.titan-text-lite-v1. To drop these, set `litellm.drop_params=True` or for proxy:\n\n`litellm_settings:\n drop_params: true`\n\n"
}
{
    "timestamp": "2025-03-05T08:35:51.815265",
    "model": "meta.llama3-8b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-8b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
{
    "timestamp": "2025-03-05T08:35:52.082592",
    "model": "meta.llama3-70b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-70b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
{
    "timestamp": "2025-03-05T08:40:11.239031",
    "model": "meta.llama3-8b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-8b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
{
    "timestamp": "2025-03-05T08:40:11.563352",
    "model": "meta.llama3-70b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-70b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
{
    "timestamp": "2025-03-05T08:57:34.072604",
    "model": "meta.llama3-8b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-8b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
{
    "timestamp": "2025-03-05T08:57:34.341214",
    "model": "meta.llama3-70b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-70b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
