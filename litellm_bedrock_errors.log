{
    "timestamp": "2025-03-05T08:59:48.531324",
    "model": "meta.llama3-8b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-8b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
{
    "timestamp": "2025-03-05T08:59:48.817031",
    "model": "meta.llama3-70b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-70b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
{
    "timestamp": "2025-03-06T12:41:16.977299",
    "model": "meta.llama3-8b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-8b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
{
    "timestamp": "2025-03-06T12:41:17.279834",
    "model": "meta.llama3-70b-instruct-v1:0",
    "error": "litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}",
    "traceback": "Traceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 458, in completion\n    response = client.post(\n        url=proxy_endpoint_url,\n    ...<2 lines>...\n        logging_obj=logging_obj,\n    )  # type: ignore\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.eu-west-2.amazonaws.com/model/meta.llama3-70b-instruct-v1:0/converse'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 2641, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 467, in completion\n    raise BedrockError(status_code=error_code, message=err.response.text)\nlitellm.llms.bedrock.common_utils.BedrockError: {\"message\":\"This model doesn't support tool use.\"}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jonesn/AI/bee/litellm-test/example2.py\", line 56, in test_model\n    json_prompt_response = litellm.completion(\n        model=model_name,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany? Please provide your answer in JSON format with a field called 'answer'.\"}],\n        response_format={\"type\": \"json_object\"} # Known issue: https://github.com/BerriAI/litellm/issues/1787 - response_format not fully respected by all models, especially in Bedrock.\n    )\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1193, in wrapper\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/utils.py\", line 1071, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/main.py\", line 3090, in completion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\n    raise e\n  File \"/Users/jonesn/AI/bee/litellm-test/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 917, in exception_type\n    raise BadRequestError(\n    ...<4 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"This model doesn't support tool use.\"}\n"
}
